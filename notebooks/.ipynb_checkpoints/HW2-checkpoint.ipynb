{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITCS 6010: Assignment #2 (V1)\n",
    "\n",
    "<font color=\"red\">(Due: 11 pm on Nov 12th) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In the middle of P125, the red line in the left figure shows that the first episode results in a change in only $V(A)$. What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/HW2_Q1.jpg\" title=\"Q1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Draw the backup diagram for SARSA and Q-learning. Compare the two algorithms. When the same amount of experience is given, what would you expect to work better than the other? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/HW2_Q2a.jpg\" title=\"Q2a\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SARSA, the current state-action pair, resulting reward, and next state-action pair are considered in each update. The actions in all steps are chosen using the current policy and therefore is considered an on-policy approach. The policy is typically an epsilon-greedy policy, and therefore it does not learn the optimal policy directly. Covergence towards the optimal policy can be accomplished by decaying eplison during the training process.\n",
    "\n",
    "In Q-learning, the current state-action pair is determined by a behavioral policy, and next state-action pair is determined by a greedy target policy. Because this approach has a seperate behaviour and target policy, we consider it an off-policy approach. Here, we can learn about the optimal policy while following an exploratory policy. Therefore, because we are learning about the optimal policy directly in Q-learning, I would expect Q-learning to work better than SARSA with the same amount of experience given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prove that the $n$-step return of SARSA in equation (7.4) can be written in terms of a novel TD error as\n",
    "$$\n",
    "G_{t:t+n} = Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{\\min(t+n, T)-1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_{t:t+n} = Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{\\min(t+n, T)-1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)]$\n",
    "<br><br>\n",
    "Expand:\n",
    "<br>\n",
    "$G_{t:t+n} = Q_{t-1}(S_t, A_t) + R_{t+1} + \\gamma Q_t(S_{t+1}, A_{t+1}) - Q_{t-1}(S_t, A_t)$<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$+ \\gamma R_{t+2} + \\gamma^2 Q_{t+1}(S_{t+2}, A_{t+2}) - \\gamma Q_{t}(S_{t+1}, A_{t+2}) + ...$<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$+ \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}) - \\gamma^{n-1} Q_{t+n-2}(S_{t+n-1}, A_{t+n-1})$\n",
    "<br><br>\n",
    "Combine into summations:\n",
    "<br>\n",
    "$G_{t:t+n} = Q_{t-1}(S_t, A_t) + R_{t+1} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} +...+ \\gamma^{n-1}R_{t+n} + \\gamma^{n}Q_{t+n-1}(S_{t+n}, A_{t+n})$<br>&emsp;&emsp;&emsp;&emsp;\n",
    "$+\\sum_{k=0}^{n-2} \\gamma^{k+1} Q_{t+k}(S_{t+k+1}, A_{t+k+1}) + \\sum_{k=-1}^{n-2} - \\gamma^{k+1} Q_{t+k}(S_{t+k+1}, A_{t+k+1})$\n",
    "<br><br>\n",
    "Cancel summation terms:\n",
    "<br>\n",
    "$G_{t:t+n} = Q_{t-1}(S_t, A_t) + R_{t+1} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} +...+ \\gamma^{n-1}R_{t+n} + \\gamma^{n}Q_{t+n-1}(S_{t+n}, A_{t+n})$<br>&emsp;&emsp;&emsp;&emsp;\n",
    "$- Q_{t-1}(S_{t}, A_{t})$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "Results in Eq. 7.4:\n",
    "<br>\n",
    "$\n",
    "G_{t:t+n} = R_{t+1} + \\gamma R_{t+1} + \\gamma^2 R_{t+2} +...+ \\gamma^{n-1}R_{t+n} + \\gamma^{n}Q_{t+n-1}(S_{t+n}, A_{t+n})\n",
    "$\n",
    "&emsp;&emsp;For $n\\geq 1, 0\\leq t < T-n$<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;To satisfy original summation restraint $\\sum_{k=t}^{\\min(t+n, T)-1}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. In P167, in both examples, the Dyna-Q+ perform better in the first phase as well as in the second phase. \n",
    "1) Why does the Dyna-Q+ work better in these two examples? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyna-Q+ performs better in both cases for the following reason. Dyna-Q does not encourage exploration in planning, and therefore it is very reluctant to change its ways once upon establishing a model. On the other hand, Dyna-Q+ encourages exploration in planning (exploration bonus $r + k\\sqrt\\tau$ ), and therefore is able to adapt more quickly than Dyna-Q to changes in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) In the second example (Figure 8.5), the difference between the two models narrowed slightly over the first part of the experiment. What is the reason for this? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference narrowed in the second case for the following reason. In the first case, when the original path was completely blocked, both methods were forced to find anouther path. For Dyna-Q, although reluctant to change its ways, eventually discovered the new path and updated its model. However, for Dyna-Q+, the exploration bonus quickly became very attractive, as it no longer could reach the goal with its current policy. Therefore, it very quickly adapted and found the new best path.\n",
    "\n",
    "In the second case, where the orginal path was left open and a new better path was allowed, Dyna-Q never found the new path. With no inititive to explore, as it has no exploratory bonus and its world model was not obstructed, it continued its usual path. Dyna-Q+ maintained its original path for some time, as it still was recieving rewards. However, after some time, the exploration bonus eventually became attactive enough to warrent exploration, and from that exploration Dyna-Q+ was able to find the new path. The gap between the two methods was smaller in this case because, without being blocked, it took longer for the exploration bonus to become attractive to Dyna-Q+, as it was still receiving reward from its old path. Therefore, the performance of Dyna-Q and Dyna-Q+ remained very similiar for a longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The parameter $\\lambda$ characterizes how fast the exponential weighting in Figure 12.2 falls off, and thus how far into the future the $\\lambda$-return algorithm looks in determining its update. But a rate factor such as $\\lambda$ is sometimes an awkward way of characterizing the speed of the decay. For some purposes, it is better to specify a time constant, or half-life. What is the equation relating $\\lambda$ and the half-life, $\\tau_\\lambda$, the time by which the weighting sequence will have fallen to half of its initial value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda^{\\tau_\\lambda} = 0.5\\lambda$$<br>\n",
    "$$\\tau_\\lambda log(\\lambda) = log(0.5\\lambda)$$<br>\n",
    "$$\\tau_\\lambda log(\\lambda) = log(0.5) + log(\\lambda)$$<br>\n",
    "$$\\tau_\\lambda log(\\lambda) - log(\\lambda) = log(0.5)$$<br>\n",
    "$$log(\\lambda)(\\tau_\\lambda - 1) = log(0.5)$$<br>\n",
    "$$\\tau_\\lambda - 1 = \\dfrac{log(0.5)}{log(\\lambda)}$$<br>\n",
    "$$\\tau_\\lambda = \\dfrac{log(0.5)}{log(\\lambda)}+1$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (Programming) Implement SARSA with neural network function approximation to solve the Maze problem. Extend the algorithm with $n$-step bootstrapping and compare the results varying $n$ to $\\{1,4,8\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maze Problem (Practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Grid Environment File\n",
    "\n",
    "Simple text file with three characters, 'O', 'H', and 'G'.\n",
    "- 'O': open space\n",
    "- 'H': Wall or obstacles\n",
    "- 'G': Goal location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOOHOOOOO\n",
      "OOOHOOHOO\n",
      "OOOOOOHOO\n",
      "OOOOHHHOO\n",
      "OOHOOOOOH\n",
      "OOHOOOGOO\n",
      "OOOOOOOOO\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cat ../grid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze example\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\" Grid World environment\n",
    "            there are four actions (left, right, up, and down) to move an agent\n",
    "            In a grid, if it reaches a goal, it get 30 points of reward.\n",
    "            If it falls in a hole or moves out of the grid world, it gets -5.\n",
    "            Each step costs -1 point. \n",
    "\n",
    "        to test GridWorld, run the following sample codes:\n",
    "\n",
    "            env = GridWorld('grid.txt')\n",
    "\n",
    "            env.print_map()\n",
    "            print [2,3], env.check_state([2,3])\n",
    "            print [0,0], env.check_state([0,0])\n",
    "            print [3,4], env.check_state([3,4])\n",
    "            print [10,3], env.check_state([10,3])\n",
    "\n",
    "            env.init([0,0])\n",
    "            print env.next(1)  # right\n",
    "            print env.next(3)  # down\n",
    "            print env.next(0)  # left\n",
    "            print env.next(2)  # up\n",
    "            print env.next(2)  # up\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "        _map        ndarray\n",
    "                    string array read from a file input\n",
    "        _size       1d array\n",
    "                    the size of _map in ndarray\n",
    "        goal_pos    tuple\n",
    "                    the index for the goal location\n",
    "        _actions    list\n",
    "                    list of actions for 4 actions\n",
    "        _s          1d array\n",
    "                    current state\n",
    "    \"\"\"\n",
    "    def __init__(self, fn):\n",
    "        # read a map from a file\n",
    "        self._map = self.read_map(fn)\n",
    "        self._size = np.asarray(self._map.shape)\n",
    "        self.goal_pos = np.where(self._map == 'G')\n",
    "\n",
    "        # definition of actions (left, right, up, and down repectively)\n",
    "        self._actions = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self._s = None\n",
    "\n",
    "    def get_cur_state(self):\n",
    "        return self._s\n",
    "\n",
    "    def get_size(self):\n",
    "        return self._size\n",
    "\n",
    "    def read_map(self, fn):\n",
    "        grid = []\n",
    "        with open(fn) as f:\n",
    "            for line in f:\n",
    "               grid.append(list(line.strip()))\n",
    "        return np.asarray(grid)\n",
    "\n",
    "    def print_map(self):\n",
    "        print( self._map )\n",
    "\n",
    "    def check_state(self, s):\n",
    "        if isinstance(s, collections.Iterable) and len(s) == 2:\n",
    "            if s[0] < 0 or s[1] < 0 or\\\n",
    "               s[0] >= self._size[0] or s[1] >= self._size[1]:\n",
    "               return 'N'\n",
    "            return self._map[tuple(s)].upper()\n",
    "        else:\n",
    "            return 'F'  # wrong input\n",
    "\n",
    "    def init(self, state=None):\n",
    "        if state is None:\n",
    "            s = [0, 0]\n",
    "        else:\n",
    "            s = state\n",
    "\n",
    "        if self.check_state(s) == 'O':\n",
    "            self._s = np.asarray(state)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid state for init\")\n",
    "\n",
    "    def next(self, a):\n",
    "        s1 = self._s + self._actions[a]\n",
    "        # state transition\n",
    "        curr = self.check_state(s1)\n",
    "        \n",
    "        if curr == 'H' or curr == 'N':\n",
    "            return -5\n",
    "        elif curr == 'F':\n",
    "            warnings.warn(\"invalid state \" + str(s1))\n",
    "            return -5\n",
    "        elif curr == 'G':\n",
    "            self._s = s1\n",
    "            return 30\n",
    "        else:\n",
    "            self._s = s1\n",
    "            return -1\n",
    "        \n",
    "    def is_goal(self):\n",
    "        return self.check_state(self._s) == 'G'\n",
    "            \n",
    "    def get_actions(self):\n",
    "        return self._actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' 'O' 'O' 'H' 'O' 'O' 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' 'H' 'O' 'O' 'H' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'H' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'H' 'H' 'H' 'O' 'O']\n",
      " ['O' 'O' 'H' 'O' 'O' 'O' 'O' 'O' 'H']\n",
      " ['O' 'O' 'H' 'O' 'O' 'O' 'G' 'O' 'O']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\"../grid.txt\")\n",
    "env.print_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-left to (0,0)\n",
    "def coord_convert(s, sz):\n",
    "    return [s[1], sz[0]-s[0]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
